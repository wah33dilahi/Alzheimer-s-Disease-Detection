import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Define dataset path and class names
dataset_path = '/content/drive/MyDrive/Alzheimer_Dataset'
classes = ["MildDemented", "ModerateDemented", "NonDemented", "VeryMildDemented"]

# Function to load images and labels
def load_data(data_dir):
    images, labels = [], []
    for label, class_name in enumerate(classes):
        class_folder = os.path.join(data_dir, class_name)
        for img_name in os.listdir(class_folder):
            img_path = os.path.join(class_folder, img_name)
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is not None:
                img = cv2.resize(img, (224, 224))
                images.append(img)
                labels.append(label)
    return np.array(images), np.array(labels)

# Load dataset
images, labels = load_data(dataset_path)
images = images / 255.0  # Normalize pixel values to [0,1]
images = np.expand_dims(images, axis=-1)  # Shape: (n, 224, 224, 1)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    images, labels, test_size=0.3, random_state=42, stratify=labels
)

print(f"Training data shape: {X_train.shape}")
print(f"Testing data shape: {X_test.shape}")


from skimage.feature import graycomatrix, graycoprops
from skimage import measure
from sklearn.preprocessing import StandardScaler

# Function to compute handcrafted features for one image
def compute_handcrafted_features(img):
    img_255 = (img * 255).astype(np.uint8)

    # Texture Features (GLCM)
    distances = [1]
    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]
    glcm = graycomatrix(img_255, distances=distances, angles=angles, symmetric=True, normed=True)
    texture_features = [
        np.mean(graycoprops(glcm, prop='contrast')),
        np.mean(graycoprops(glcm, prop='dissimilarity')),
        np.mean(graycoprops(glcm, prop='homogeneity')),
        np.mean(graycoprops(glcm, prop='energy')),
        np.mean(graycoprops(glcm, prop='correlation')),
        np.mean(graycoprops(glcm, prop='ASM'))
    ]

    # Shape & Morphological Features
    thresh_val = np.mean(img_255)
    _, binary_img = cv2.threshold(img_255, thresh_val, 255, cv2.THRESH_BINARY)
    binary_img_bool = binary_img.astype(bool)
    labels_img = measure.label(binary_img_bool)
    regions = measure.regionprops(labels_img)
    if regions:
        region = max(regions, key=lambda r: r.area)
        shape_features = [region.area, region.perimeter, region.eccentricity, region.solidity]
    else:
        shape_features = [0, 0, 0, 0]

    # Intensity Histogram (32 bins)
    hist, _ = np.histogram(img_255.ravel(), bins=32, range=(0, 256))
    hist = hist.astype("float") / (hist.sum() + 1e-7)

    # Combine features into one vector
    feature_vector = texture_features + shape_features + hist.tolist()
    return np.array(feature_vector)

# Precompute handcrafted features for training and testing sets
handcrafted_train = np.array([compute_handcrafted_features(img.squeeze()) for img in X_train])
handcrafted_test  = np.array([compute_handcrafted_features(img.squeeze()) for img in X_test])

scaler = StandardScaler()
handcrafted_train = scaler.fit_transform(handcrafted_train)
handcrafted_test = scaler.transform(handcrafted_test)


import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Concatenate, Lambda, BatchNormalization
from tensorflow.keras.optimizers import Adam

deep_input = Input(shape=(224, 224, 1), name="deep_input")
rgb_layer = Lambda(lambda x: tf.image.grayscale_to_rgb(x))(deep_input)

# ResNet50 Branch
base_resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
base_resnet.trainable = False
resnet_features = base_resnet(rgb_layer)
resnet_features = Flatten()(resnet_features)
resnet_features = Dense(256, activation='relu')(resnet_features)
resnet_features = BatchNormalization()(resnet_features)
resnet_features = Dropout(0.5)(resnet_features)

# Custom CNN Branch
custom_cnn = Sequential(name="custom_cnn")
custom_cnn.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 1)))
custom_cnn.add(MaxPooling2D(pool_size=(2, 2)))
custom_cnn.add(Dropout(0.2))
custom_cnn.add(Flatten())
custom_cnn.add(Dense(128, activation='relu'))
custom_cnn.add(BatchNormalization())
custom_features = custom_cnn(deep_input)

# Fusion of Deep Features
deep_merged = Concatenate()([resnet_features, custom_features])
deep_merged = Dense(128, activation='relu')(deep_merged)
deep_merged = BatchNormalization()(deep_merged)
deep_merged = Dropout(0.5)(deep_merged)

# Handcrafted Features
hc_input = Input(shape=(handcrafted_train.shape[1],), name="handcrafted_input")
hc_features = Dense(64, activation='relu')(hc_input)
hc_features = BatchNormalization()(hc_features)
hc_features = Dropout(0.3)(hc_features)

# Final Fusion
final_merged = Concatenate()([deep_merged, hc_features])
final_merged = Dense(128, activation='relu')(final_merged)
final_merged = Dropout(0.5)(final_merged)
output = Dense(len(classes), activation='softmax')(final_merged)

optimizer = Adam(learning_rate=5e-5, clipnorm=1.0)
model = Model(inputs=[deep_input, hc_input], outputs=output)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()


history = model.fit(
    [X_train, handcrafted_train], y_train,
    epochs=50, batch_size=32,
    validation_data=([X_test, handcrafted_test], y_test)
)



print("Final Training Accuracy:", history.history['accuracy'][-1])
print("Final Validation Accuracy:", history.history['val_accuracy'][-1])



import matplotlib.pyplot as plt

def plot_training_history(history):
    # Plot Accuracy
    plt.figure(figsize=(14, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    plt.title('Model Accuracy Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)

    # Plot Loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss', marker='o')
    plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
    plt.title('Model Loss Over Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()

# Call the function after training
plot_training_history(history)
